{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fc419-de48-4675-8805-dc4a3b96c0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7a7c3-85ff-4cb9-8d68-826934af0c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Balanced dataset with 2149 samples per class, total = 10745\n",
      "Symptoms: ['pain', 'cough', 'edema', 'fever', 'nausea']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2daee024b73c423ea83c05cd323a4995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1249aef91435402a9f0c4e6103cb3d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a6874e07d74b4a8072236df99b10a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20a85f0dfc744c8bbd3785a04033db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1075 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962a8a273a934d939943b5254ed918be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f4a4d77bd74dc5b5e94499654304fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base | Epoch 1/5 - Loss: 0.4660 - Acc: 0.9293 P: 0.9306 R: 0.9294 F1: 0.9288 AUROC: 0.9867\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# === CONFIG ===\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 16\n",
    "max_length = 256\n",
    "epochs = 5\n",
    "top_K = 5\n",
    "\n",
    "# === STEP 1: Symptom Lexicon ===\n",
    "symptom_lexicon = list(set([\n",
    "    \"fever\", \"cough\", \"headache\", \"nausea\", \"vomiting\", \"fatigue\", \"chest pain\", \"shortness of breath\",\n",
    "    \"abdominal pain\", \"dizziness\", \"diarrhea\", \"constipation\", \"joint pain\", \"back pain\", \"depression\", \"anxiety\",\n",
    "    \"rash\", \"itching\", \"seizure\", \"confusion\", \"palpitations\", \"insomnia\", \"loss of appetite\", \"urinary frequency\",\n",
    "    \"chills\", \"syncope\", \"sore throat\", \"swelling\", \"pain\", \"malaise\", \"cramps\", \"numbness\", \"tingling\",\n",
    "    \"blurry vision\", \"weakness\", \"edema\", \"hallucinations\", \"bleeding\", \"difficulty breathing\", \"burning\"\n",
    "]))\n",
    "\n",
    "# === STEP 2: Load Notes and Extract Top-K Symptoms ===\n",
    "notes = pd.read_csv(\"NOTEEVENTS_random.csv\", usecols=[\"TEXT\"]).dropna()\n",
    "notes = notes.sample(frac=0.1, random_state=42)\n",
    "notes[\"TEXT\"] = notes[\"TEXT\"].str.lower().str.slice(0, 1000)\n",
    "\n",
    "def extract_label(text):\n",
    "    for s in symptom_lexicon:\n",
    "        if re.search(rf\"\\b{re.escape(s)}\\b\", text):\n",
    "            return s\n",
    "    return None\n",
    "\n",
    "notes[\"symptom\"] = notes[\"TEXT\"].apply(extract_label)\n",
    "notes = notes.dropna(subset=[\"symptom\"])\n",
    "\n",
    "# Top-K symptoms\n",
    "top_symptoms = Counter(notes[\"symptom\"]).most_common(top_K)\n",
    "symptom_list = [s for s, _ in top_symptoms]\n",
    "#symptom_list = []\n",
    "symptom2id = {s: i for i, s in enumerate(symptom_list)}\n",
    "id2symptom = {i: s for s, i in symptom2id.items()}\n",
    "\n",
    "# === STEP 3: Balance Dataset ===\n",
    "balanced = []\n",
    "min_count = min([sum(notes[\"symptom\"] == s) for s in symptom_list])\n",
    "for s in symptom_list:\n",
    "    subset = notes[notes[\"symptom\"] == s].sample(n=min_count, random_state=42)\n",
    "    balanced.append(subset)\n",
    "\n",
    "balanced_df = pd.concat(balanced).reset_index(drop=True)\n",
    "balanced_df = balanced_df.rename(columns={\"TEXT\": \"text\", \"symptom\": \"label\"})\n",
    "balanced_df[\"label\"] = balanced_df[\"label\"].map(symptom2id)\n",
    "\n",
    "print(f\"✅ Balanced dataset with {min_count} samples per class, total = {len(balanced_df)}\")\n",
    "print(\"Symptoms:\", symptom_list)\n",
    "\n",
    "# === STEP 4: Prepare HuggingFace Datasets ===\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.1, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "eval_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "eval_dataset = eval_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# === STEP 5: Evaluation Function with Per-Symptom Metrics ===\n",
    "def evaluate_model(model, loader, method_name=None, epoch=None):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            labels = batch[\"label\"].cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(np.eye(len(symptom_list))[all_labels], np.array(all_probs), average=\"macro\", multi_class=\"ovr\")\n",
    "    except Exception as e:\n",
    "        print(\"AUROC error:\", str(e))\n",
    "        auroc = 0.0\n",
    "\n",
    "    if method_name and epoch is not None:\n",
    "        per_class_metrics = precision_recall_fscore_support(all_labels, all_preds, labels=list(range(len(symptom_list))), zero_division=0)\n",
    "        try:\n",
    "            per_class_auroc = [\n",
    "                roc_auc_score((np.array(all_labels) == i).astype(int), np.array(all_probs)[:, i])\n",
    "                for i in range(len(symptom_list))\n",
    "            ]\n",
    "        except:\n",
    "            per_class_auroc = [0.0] * len(symptom_list)\n",
    "\n",
    "        csv_file = f\"per_symptom_{method_name}_metrics.csv\"\n",
    "        write_header = not os.path.exists(csv_file)\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow([\"Epoch\", \"Symptom\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"])\n",
    "            for i in range(len(symptom_list)):\n",
    "                writer.writerow([\n",
    "                    epoch,\n",
    "                    id2symptom[i],\n",
    "                    per_class_metrics[0][i],\n",
    "                    per_class_metrics[1][i],\n",
    "                    per_class_metrics[2][i],\n",
    "                    per_class_auroc[i]\n",
    "                ])\n",
    "\n",
    "    return acc, precision, recall, f1, auroc\n",
    "\n",
    "# === STEP 6: Training ===\n",
    "def train_model(model, method_name, lr):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=epochs * len(train_loader))\n",
    "    csv_file = f\"multiclass_{method_name}_metrics.csv\"\n",
    "    with open(csv_file, mode=\"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"Epoch\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"])\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            loss = F.cross_entropy(outputs.logits, batch[\"label\"].long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        acc, prec, rec, f1, auroc = evaluate_model(model, eval_loader, method_name, epoch)\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([epoch, acc, prec, rec, f1, auroc])\n",
    "        print(f\"{method_name} | Epoch {epoch}/{epochs} - Loss: {total_loss / len(train_loader):.4f} - Acc: {acc:.4f} P: {prec:.4f} R: {rec:.4f} F1: {f1:.4f} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# === STEP 7: Models ===\n",
    "def base_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "   # optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    train_model(model, \"base\",lr=3e-5) \n",
    "\n",
    "def pruning_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=0.3)\n",
    "    train_model(model, \"pruning\", lr=2e-5)\n",
    "\n",
    "def lowrank_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "\n",
    "    # Apply low-rank SVD to only the classifier layer\n",
    "    classifier = model.classifier\n",
    "    weight = classifier.weight.data\n",
    "    rank = min(32, weight.size(1) // 2)  # tunable rank\n",
    "\n",
    "    try:\n",
    "        # Compute low-rank SVD approximation\n",
    "        u, s, v = torch.svd_lowrank(weight, q=rank)\n",
    "        lowrank_weight = (u @ torch.diag(s) @ v.t()).to(weight.device)\n",
    "\n",
    "        # Replace original weight with low-rank approximation\n",
    "        classifier.weight.data.copy_(lowrank_weight)\n",
    "\n",
    "        print(f\"✅ Applied low-rank SVD to classifier with rank {rank}\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ SVD low-rank approximation failed:\", e)\n",
    "\n",
    "    # Use higher LR to adapt the modified layer\n",
    "  #  optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    train_model(model, \"lowrank\",  lr=5e-5)\n",
    "\n",
    "\n",
    "def quantization_model():\n",
    "    from torch.quantization import get_default_qat_qconfig, prepare_qat, convert\n",
    "\n",
    "    # Load and prepare model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list))\n",
    "    model.train()\n",
    "    model.qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "\n",
    "    # Only QAT on encoder (to avoid issues with LayerNorm)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.qconfig = model.qconfig\n",
    "\n",
    "    # Prepare QAT model\n",
    "    model_prepared = prepare_qat(model)\n",
    "    model_prepared.to(device)\n",
    "\n",
    "    # Train QAT model\n",
    "    train_model(model_prepared, \"qat\", lr=2e-5)\n",
    "\n",
    "    # Convert to quantized version after training\n",
    "    model_quantized = convert(model_prepared.eval().cpu())\n",
    "    model_quantized.to(device)\n",
    "\n",
    "    # Optional: Evaluate final quantized model\n",
    "    acc, prec, rec, f1, auroc = evaluate_model(model_quantized, eval_loader, \"qat-final\", epoch=\"final\")\n",
    "    print(f\"QAT Final — Acc: {acc:.4f}, F1: {f1:.4f}, AUROC: {auroc:.4f}\")\n",
    "\n",
    "\n",
    "def distillation_model():\n",
    "    teacher = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "    teacher.eval()\n",
    "\n",
    "    # Smaller student (or same if just testing)\n",
    "    student = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=5e-5)  # slightly higher LR for student\n",
    "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "    T = 4.0  # temperature\n",
    "    alpha = 0.5  # balance CE and KD loss\n",
    "\n",
    "    csv_file = \"multiclass_distillation_metrics-small-dataset.csv\"\n",
    "    with open(csv_file, mode=\"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"Epoch\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"])\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        student.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "\n",
    "            student_logits = student(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "\n",
    "            ce_loss = F.cross_entropy(student_logits, batch[\"label\"].long())\n",
    "            kd_loss = F.kl_div(\n",
    "                F.log_softmax(student_logits / T, dim=-1),\n",
    "                F.softmax(teacher_logits / T, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            ) * (T * T)\n",
    "\n",
    "            loss = alpha * ce_loss + (1 - alpha) * kd_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        acc, prec, rec, f1, auroc = evaluate_model(student, eval_loader, \"distillation\", epoch)\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([epoch, acc, prec, rec, f1, auroc])\n",
    "        print(f\"Distill | Epoch {epoch}/{epochs} - Loss: {total_loss/len(train_loader):.4f} \"\n",
    "              f\"- Acc: {acc:.4f} P: {prec:.4f} R: {rec:.4f} F1: {f1:.4f} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# === STEP 8: Run All Models ===\n",
    "def run_all():\n",
    "    base_model()\n",
    "    pruning_model()\n",
    "    lowrank_model()\n",
    "    distillation_model()\n",
    "    quantization_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f5938-90f9-4b2d-b672-73804b3e0e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a80aa2-b991-4952-ba08-d146411e9c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58631bc-747a-42a3-905c-73251ced6204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
