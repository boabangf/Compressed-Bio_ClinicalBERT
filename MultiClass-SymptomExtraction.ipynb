{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fc419-de48-4675-8805-dc4a3b96c0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7a7c3-85ff-4cb9-8d68-826934af0c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Balanced dataset with 870 samples per class, total = 4350\n",
      "Symptoms: ['edema', 'pain', 'cough', 'fever', 'bleeding']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a107a5cedc7b44ceb8f24be0b9edcd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4cb8db9c7641d68a3639442fad41b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/870 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Applied low-rank SVD to classifier with rank 32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# === CONFIG ===\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 16\n",
    "max_length = 256\n",
    "epochs = 20\n",
    "top_K = 5\n",
    "\n",
    "# === STEP 1: Symptom Lexicon ===\n",
    "symptom_lexicon = list(set([\n",
    "    \"fever\", \"cough\", \"headache\", \"nausea\", \"vomiting\", \"fatigue\", \"chest pain\", \"shortness of breath\",\n",
    "    \"abdominal pain\", \"dizziness\", \"diarrhea\", \"constipation\", \"joint pain\", \"back pain\", \"depression\", \"anxiety\",\n",
    "    \"rash\", \"itching\", \"seizure\", \"confusion\", \"palpitations\", \"insomnia\", \"loss of appetite\", \"urinary frequency\",\n",
    "    \"chills\", \"syncope\", \"sore throat\", \"swelling\", \"pain\", \"malaise\", \"cramps\", \"numbness\", \"tingling\",\n",
    "    \"blurry vision\", \"weakness\", \"edema\", \"hallucinations\", \"bleeding\", \"difficulty breathing\", \"burning\"\n",
    "]))\n",
    "\n",
    "# === STEP 2: Load Notes and Extract Top-K Symptoms ===\n",
    "notes = pd.read_csv(\"NOTEEVENTS_random.csv\", usecols=[\"TEXT\"]).dropna()\n",
    "notes = notes.sample(frac=0.1, random_state=42)\n",
    "notes[\"TEXT\"] = notes[\"TEXT\"].str.lower().str.slice(0, 1000)\n",
    "\n",
    "def extract_label(text):\n",
    "    for s in symptom_lexicon:\n",
    "        if re.search(rf\"\\b{re.escape(s)}\\b\", text):\n",
    "            return s\n",
    "    return None\n",
    "\n",
    "notes[\"symptom\"] = notes[\"TEXT\"].apply(extract_label)\n",
    "notes = notes.dropna(subset=[\"symptom\"])\n",
    "\n",
    "# Top-K symptoms\n",
    "top_symptoms = Counter(notes[\"symptom\"]).most_common(top_K)\n",
    "symptom_list = [s for s, _ in top_symptoms]\n",
    "#symptom_list = []\n",
    "symptom2id = {s: i for i, s in enumerate(symptom_list)}\n",
    "id2symptom = {i: s for s, i in symptom2id.items()}\n",
    "\n",
    "# === STEP 3: Balance Dataset ===\n",
    "balanced = []\n",
    "min_count = min([sum(notes[\"symptom\"] == s) for s in symptom_list])\n",
    "for s in symptom_list:\n",
    "    subset = notes[notes[\"symptom\"] == s].sample(n=min_count, random_state=42)\n",
    "    balanced.append(subset)\n",
    "\n",
    "balanced_df = pd.concat(balanced).reset_index(drop=True)\n",
    "balanced_df = balanced_df.rename(columns={\"TEXT\": \"text\", \"symptom\": \"label\"})\n",
    "balanced_df[\"label\"] = balanced_df[\"label\"].map(symptom2id)\n",
    "\n",
    "print(f\"✅ Balanced dataset with {min_count} samples per class, total = {len(balanced_df)}\")\n",
    "print(\"Symptoms:\", symptom_list)\n",
    "\n",
    "# === STEP 4: Prepare HuggingFace Datasets ===\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.2, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "eval_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "eval_dataset = eval_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# === STEP 5: Evaluation Function with Per-Symptom Metrics ===\n",
    "def evaluate_model(model, loader, method_name=None, epoch=None):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            labels = batch[\"label\"].cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(np.eye(len(symptom_list))[all_labels], np.array(all_probs), average=\"macro\", multi_class=\"ovr\")\n",
    "    except Exception as e:\n",
    "        print(\"AUROC error:\", str(e))\n",
    "        auroc = 0.0\n",
    "\n",
    "    if method_name and epoch is not None:\n",
    "        per_class_metrics = precision_recall_fscore_support(all_labels, all_preds, labels=list(range(len(symptom_list))), zero_division=0)\n",
    "        try:\n",
    "            per_class_auroc = [\n",
    "                roc_auc_score((np.array(all_labels) == i).astype(int), np.array(all_probs)[:, i])\n",
    "                for i in range(len(symptom_list))\n",
    "            ]\n",
    "        except:\n",
    "            per_class_auroc = [0.0] * len(symptom_list)\n",
    "\n",
    "        csv_file = f\"per_symptom_{method_name}_metrics.csv\"\n",
    "        write_header = not os.path.exists(csv_file)\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            if write_header:\n",
    "                writer.writerow([\"Epoch\", \"Symptom\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"])\n",
    "            for i in range(len(symptom_list)):\n",
    "                writer.writerow([\n",
    "                    epoch,\n",
    "                    id2symptom[i],\n",
    "                    per_class_metrics[0][i],\n",
    "                    per_class_metrics[1][i],\n",
    "                    per_class_metrics[2][i],\n",
    "                    per_class_auroc[i]\n",
    "                ])\n",
    "\n",
    "    return acc, precision, recall, f1, auroc\n",
    "\n",
    "# === STEP 6: Training ===\n",
    "def train_model(model, method_name, lr):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=epochs * len(train_loader))\n",
    "    csv_file = f\"multiclass_{method_name}_metrics.csv\"\n",
    "    with open(csv_file, mode=\"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"Epoch\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"])\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            loss = F.cross_entropy(outputs.logits, batch[\"label\"].long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        acc, prec, rec, f1, auroc = evaluate_model(model, eval_loader, method_name, epoch)\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([epoch, acc, prec, rec, f1, auroc])\n",
    "        print(f\"{method_name} | Epoch {epoch}/{epochs} - Loss: {total_loss / len(train_loader):.4f} - Acc: {acc:.4f} P: {prec:.4f} R: {rec:.4f} F1: {f1:.4f} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# === STEP 7: Models ===\n",
    "def base_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "   # optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    train_model(model, \"base\",lr=2e-5) \n",
    "\n",
    "def pruning_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    for _, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=0.3)\n",
    "    train_model(model, \"pruning\", lr=2e-5)\n",
    "\n",
    "def lowrank_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "\n",
    "    # Apply low-rank SVD to only the classifier layer\n",
    "    classifier = model.classifier\n",
    "    weight = classifier.weight.data\n",
    "    rank = min(32, weight.size(1) // 2)  # tunable rank\n",
    "\n",
    "    try:\n",
    "        # Compute low-rank SVD approximation\n",
    "        u, s, v = torch.svd_lowrank(weight, q=rank)\n",
    "        lowrank_weight = (u @ torch.diag(s) @ v.t()).to(weight.device)\n",
    "\n",
    "        # Replace original weight with low-rank approximation\n",
    "        classifier.weight.data.copy_(lowrank_weight)\n",
    "\n",
    "        print(f\"✅ Applied low-rank SVD to classifier with rank {rank}\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ SVD low-rank approximation failed:\", e)\n",
    "\n",
    "    # Use higher LR to adapt the modified layer\n",
    "  #  optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    train_model(model, \"lowrank\",  lr=5e-5)\n",
    "\n",
    "\n",
    "def quantization_model():\n",
    "    from torch.quantization import get_default_qat_qconfig, prepare_qat, convert\n",
    "\n",
    "    # Load and prepare model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list))\n",
    "    model.train()\n",
    "    model.qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "\n",
    "    # Only QAT on encoder (to avoid issues with LayerNorm)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.qconfig = model.qconfig\n",
    "\n",
    "    # Prepare QAT model\n",
    "    model_prepared = prepare_qat(model)\n",
    "    model_prepared.to(device)\n",
    "\n",
    "    # Train QAT model\n",
    "    train_model(model_prepared, \"qat\", lr=2e-5)\n",
    "\n",
    "    # Convert to quantized version after training\n",
    "    model_quantized = convert(model_prepared.eval().cpu())\n",
    "    model_quantized.to(device)\n",
    "\n",
    "    # Optional: Evaluate final quantized model\n",
    "    acc, prec, rec, f1, auroc = evaluate_model(model_quantized, eval_loader, \"qat-final\", epoch=\"final\")\n",
    "    print(f\"QAT Final — Acc: {acc:.4f}, F1: {f1:.4f}, AUROC: {auroc:.4f}\")\n",
    "\n",
    "\n",
    "def distillation_model():\n",
    "    teacher = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "    teacher.eval()\n",
    "\n",
    "    # Smaller student (or same if just testing)\n",
    "    student = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=5e-5)  # slightly higher LR for student\n",
    "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "    T = 4.0  # temperature\n",
    "    alpha = 0.5  # balance CE and KD loss\n",
    "\n",
    "    csv_file = \"multiclass_distillation_metrics.csv\"\n",
    "    with open(csv_file, mode=\"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"Epoch\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"])\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        student.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "\n",
    "            student_logits = student(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "\n",
    "            ce_loss = F.cross_entropy(student_logits, batch[\"label\"].long())\n",
    "            kd_loss = F.kl_div(\n",
    "                F.log_softmax(student_logits / T, dim=-1),\n",
    "                F.softmax(teacher_logits / T, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            ) * (T * T)\n",
    "\n",
    "            loss = alpha * ce_loss + (1 - alpha) * kd_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        acc, prec, rec, f1, auroc = evaluate_model(student, eval_loader, \"distillation\", epoch)\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow([epoch, acc, prec, rec, f1, auroc])\n",
    "        print(f\"Distill | Epoch {epoch}/{epochs} - Loss: {total_loss/len(train_loader):.4f} \"\n",
    "              f\"- Acc: {acc:.4f} P: {prec:.4f} R: {rec:.4f} F1: {f1:.4f} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# === STEP 8: Run All Models ===\n",
    "def run_all():\n",
    "  #  base_model()\n",
    "   # pruning_model()\n",
    "    lowrank_model()\n",
    "   # distillation_model()\n",
    "   # quantization_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "505f5938-90f9-4b2d-b672-73804b3e0e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m233.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed safetensors-0.5.3 tokenizers-0.21.2 transformers-4.53.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55a80aa2-b991-4952-ba08-d146411e9c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1\n",
      "\u001b[2K  Attempting uninstall: dill\n",
      "\u001b[2K    Found existing installation: dill 0.4.0\n",
      "\u001b[2K    Uninstalling dill-0.4.0:\n",
      "\u001b[2K      Successfully uninstalled dill-0.4.0\n",
      "\u001b[2K  Attempting uninstall: multiprocess0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [dill]\n",
      "\u001b[2K    Found existing installation: multiprocess 0.70.18━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [dill]\n",
      "\u001b[2K    Uninstalling multiprocess-0.70.18:m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [dill]\n",
      "\u001b[2K      Successfully uninstalled multiprocess-0.70.18━━━━━━━━━━━\u001b[0m \u001b[32m2/5\u001b[0m [dill]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [datasets]4/5\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.4 requires dill>=0.4.0, but you have dill 0.3.8 which is incompatible.\n",
      "pathos 0.3.4 requires multiprocess>=0.70.18, but you have multiprocess 0.70.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.0.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58631bc-747a-42a3-905c-73251ced6204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
