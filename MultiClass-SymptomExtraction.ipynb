{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719fc419-de48-4675-8805-dc4a3b96c0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42dd6b56071d44df921e9cddd579fcd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69578bdd89c74eee8d2178c594c7d22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base | Epoch 1/5 - Loss: 1.3899 - Acc: 0.4898 P: 0.0980 R: 0.2000 F1: 0.1315 AUROC: 0.6835\n",
      "base | Epoch 2/5 - Loss: 1.2898 - Acc: 0.4898 P: 0.0980 R: 0.2000 F1: 0.1315 AUROC: 0.8009\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# === Config ===\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 16\n",
    "max_length = 256\n",
    "epochs = 5\n",
    "symptom_list = [\"fever\", \"cough\", \"headache\", \"nausea\", \"vomiting\"]\n",
    "symptom2id = {s: i for i, s in enumerate(symptom_list)}\n",
    "id2symptom = {i: s for s, i in symptom2id.items()}\n",
    "\n",
    "# === Load and preprocess data ===\n",
    "notes = pd.read_csv(\"NOTEEVENTS_random_chatgpt.csv\").dropna(subset=[\"TEXT\"])\n",
    "notes = notes.sample(frac=0.3, random_state=42)\n",
    "notes[\"TEXT\"] = notes[\"TEXT\"].str.slice(0, 1000)\n",
    "\n",
    "def assign_symptom_label(text):\n",
    "    text = text.lower()\n",
    "    for s in symptom_list:\n",
    "        if s in text:\n",
    "            return symptom2id[s]\n",
    "    return None\n",
    "\n",
    "notes[\"label\"] = notes[\"TEXT\"].apply(assign_symptom_label)\n",
    "notes = notes.dropna(subset=[\"label\"])\n",
    "\n",
    "df = notes[[\"TEXT\", \"label\"]].rename(columns={\"TEXT\": \"text\", \"label\": \"labels\"})\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42, stratify=df[\"labels\"])\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "eval_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# === Tokenization ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "eval_dataset = eval_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# === Evaluation ===\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "            preds = np.argmax(probs, axis=1)\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_labels, np.array(all_probs), multi_class=\"ovr\", average=\"macro\")\n",
    "    except:\n",
    "        auroc = 0.0\n",
    "\n",
    "    return acc, precision, recall, f1, auroc\n",
    "\n",
    "# === Training ===\n",
    "def train_model(model, method_name):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = get_scheduler(\n",
    "        \"cosine\", \n",
    "        optimizer=optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=epochs * len(train_loader)\n",
    "    )\n",
    "\n",
    "    csv_file = f\"multiclass_{method_name}_metrics.csv\"\n",
    "    with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Epoch\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"])\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            \n",
    "            labels = batch[\"labels\"].long()  # Fix label dtype for cross_entropy\n",
    "\n",
    "            loss = F.cross_entropy(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        acc, prec, rec, f1, auroc = evaluate_model(model, eval_loader)\n",
    "\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch, acc, prec, rec, f1, auroc])\n",
    "\n",
    "        print(\n",
    "            f\"{method_name} | Epoch {epoch}/{epochs} - Loss: {total_loss / len(train_loader):.4f} \"\n",
    "            f\"- Acc: {acc:.4f} P: {prec:.4f} R: {rec:.4f} F1: {f1:.4f} AUROC: {auroc:.4f}\"\n",
    "        )\n",
    "\n",
    "# === Models ===\n",
    "def base_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "    train_model(model, \"base\")\n",
    "\n",
    "def pruning_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=0.3)\n",
    "    train_model(model, \"pruning\")\n",
    "\n",
    "def lowrank_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            try:\n",
    "                u, s, v = torch.svd_lowrank(module.weight.data, q=8)\n",
    "                module.weight.data.copy_((u @ torch.diag(s) @ v.t()).to(module.weight.device))\n",
    "            except:\n",
    "                pass\n",
    "    train_model(model, \"lowrank\")\n",
    "\n",
    "def quantization_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list))\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "    model_q = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "    model_q.to(device)\n",
    "    train_model(model_q, \"quantization\")\n",
    "\n",
    "def distillation_model():\n",
    "    teacher = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "    teacher.eval()\n",
    "    student = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(symptom_list)).to(device)\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=2e-5)\n",
    "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "    csv_file = \"multiclass_distillation_metrics.csv\"\n",
    "    with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Epoch\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"])\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        student.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            student_logits = student(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            ce_loss = F.cross_entropy(student_logits, batch[\"labels\"].long())\n",
    "            kd_loss = F.kl_div(\n",
    "                F.log_softmax(student_logits / 2.0, dim=-1),\n",
    "                F.softmax(teacher_logits / 2.0, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            ) * 4.0\n",
    "            loss = 0.1 * ce_loss + 0.9 * kd_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        acc, prec, rec, f1, auroc = evaluate_model(student, eval_loader)\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch, acc, prec, rec, f1, auroc])\n",
    "        print(f\"Distillation | Epoch {epoch}/{epochs} - Loss: {total_loss/len(train_loader):.4f} \"\n",
    "              f\"- Acc: {acc:.4f} P: {prec:.4f} R: {rec:.4f} F1: {f1:.4f} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# === Run all ===\n",
    "def run_all():\n",
    "    base_model()\n",
    "    pruning_model()\n",
    "    lowrank_model()\n",
    "    distillation_model()\n",
    "    quantization_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505f5938-90f9-4b2d-b672-73804b3e0e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
