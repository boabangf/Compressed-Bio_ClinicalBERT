{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fc654-bbb0-47e3-b099-3dc9c8d5a3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44aaccd3cdcf455f96f79fc0471dd96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7116c26d0b314b388d8ddda6a70cf6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# === Configuration ===\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 16\n",
    "max_length = 256\n",
    "epochs = 5\n",
    "symptom = \"fever\"\n",
    "\n",
    "# === Load and preprocess data ===\n",
    "notes = pd.read_csv(\"NOTEEVENTS_random_chatgpt.csv\")\n",
    "notes = notes.dropna(subset=[\"TEXT\"])\n",
    "notes = notes.sample(frac=1, random_state=42)\n",
    "notes[\"TEXT\"] = notes[\"TEXT\"].str.slice(0, 1000)\n",
    "notes[\"label\"] = notes[\"TEXT\"].str.lower().str.contains(symptom.lower()).astype(int)\n",
    "\n",
    "train_df, test_df = train_test_split(notes[[\"TEXT\", \"label\"]].rename(columns={\"TEXT\": \"text\", \"label\": \"labels\"}), test_size=0.1, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "eval_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "# === Tokenization ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "eval_dataset = eval_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size)\n",
    "\n",
    "# === Evaluation ===\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_probs.extend(probs)\n",
    "            all_labels.extend(labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", zero_division=0)\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_labels, all_probs)\n",
    "    except:\n",
    "        auroc = 0.0\n",
    "    return precision, recall, f1, auroc\n",
    "\n",
    "# === Training ===\n",
    "def train_model(model, method_name):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=epochs * len(train_loader))\n",
    "    csv_file = f\"symptom_{method_name}_metrics.csv\"\n",
    "    with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Epoch\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"])\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            loss = F.binary_cross_entropy_with_logits(outputs.logits.squeeze(), batch[\"labels\"].float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        precision, recall, f1, auroc = evaluate_model(model, eval_loader)\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch, precision, recall, f1, auroc])\n",
    "        print(f\"{method_name} | Epoch {epoch}/{epochs} - Loss: {total_loss/len(train_loader):.4f} - P: {precision:.4f} R: {recall:.4f} F1: {f1:.4f} AUROC: {auroc:.4f}\")\n",
    "\n",
    "# === Models ===\n",
    "def base_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)\n",
    "    train_model(model, \"base\")\n",
    "\n",
    "def pruning_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=0.3)\n",
    "    train_model(model, \"pruning\")\n",
    "\n",
    "def lowrank_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            weight = module.weight.data\n",
    "            try:\n",
    "                u, s, v = torch.svd_lowrank(weight, q=8)\n",
    "                module.weight.data.copy_((u @ torch.diag(s) @ v.t()).to(weight.device))\n",
    "            except:\n",
    "                pass\n",
    "    train_model(model, \"lowrank\")\n",
    "\n",
    "def quantization_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "    model_quantized = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "    model_quantized.to(device)\n",
    "    train_model(model_quantized, \"quantization\")\n",
    "\n",
    "def distillation_model():\n",
    "    teacher = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)\n",
    "    teacher.eval()\n",
    "    student = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)\n",
    "    optimizer = torch.optim.AdamW(student.parameters(), lr=2e-5)\n",
    "    scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=epochs * len(train_loader))\n",
    "\n",
    "    csv_file = \"symptom_distillation_metrics.csv\"\n",
    "    with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Epoch\", \"Precision\", \"Recall\", \"F1\", \"AUROC\"])\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        student.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            student_logits = student(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits\n",
    "            ce_loss = F.binary_cross_entropy_with_logits(student_logits.squeeze(), batch[\"labels\"].float())\n",
    "            kd_loss = F.kl_div(\n",
    "                F.logsigmoid(student_logits.squeeze()),\n",
    "                torch.sigmoid(teacher_logits.squeeze()),\n",
    "                reduction=\"batchmean\"\n",
    "            )\n",
    "            loss = 0.1 * ce_loss + 0.9 * kd_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        precision, recall, f1, auroc = evaluate_model(student, eval_loader)\n",
    "        with open(csv_file, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([epoch, precision, recall, f1, auroc])\n",
    "        print(f\"Distillation | Epoch {epoch}/{epochs} - Loss: {total_loss/len(train_loader):.4f} - P: {precision:.4f} R: {recall:.4f} F1: {f1:.4f} AUROC: {auroc:.4f}\")\n",
    "\n",
    "def run_all():\n",
    "    base_model()\n",
    "    pruning_model()\n",
    "    lowrank_model()\n",
    "    distillation_model()\n",
    "    quantization_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b8bb8-89ca-4b34-b223-6cd22072c408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
